{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonzhang/Documents/PersonalProjects/twitter_dashboard/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 929/929 [00:00<00:00, 277kB/s]\n",
      "Downloading: 100%|██████████| 501M/501M [00:39<00:00, 12.7MB/s] \n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.76MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.22MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 41.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "\"\"\"\n",
    "Sentiment Analysis model and tokenizer\n",
    "\"\"\"\n",
    "\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Labels:\n",
    "* 0: Negative\n",
    "* 1: Neutral\n",
    "* 2: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def roberta_preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess tweet text for the model\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word.startswith(\"@\") and len(word) > 1:\n",
    "            word = \"@user\"\n",
    "        elif word.startswith(\"http\"):\n",
    "            word = \"http\"\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def roberta_sentiment(text: str):\n",
    "    # labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    processed_text = roberta_preprocess(text)\n",
    "    # sentiment analysis\n",
    "    encoded_tweet = tokenizer(processed_text, return_tensors=\"pt\")\n",
    "    output = model(**encoded_tweet)\n",
    "\n",
    "    # Convert output pytorch tensor to numpy array by detaching the computational graph\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ind = argmax(scores)\n",
    "\n",
    "    # for label, score in zip(labels, scores):\n",
    "        # print(f'\\t{label}: {score}')\n",
    "        # pass\n",
    "\n",
    "    # sentiment = labels[ind]\n",
    "    # return sentiment\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_sentiment(\"Hello there amazing friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food was great!\n",
      "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentence = \"The food was great!\"\n",
    "vs = analyzer.polarity_scores(sentence)\n",
    "# print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "print(sentence)\n",
    "print(vs)\n",
    "print(argmax(list(vs.values())[:3]))\n",
    "\n",
    "def vader_sentiment(text: str):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    # Drop compound score\n",
    "    # print(scores)\n",
    "    return argmax(list(scores.values())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiment(\"average car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-04 14:54:08,524 loading file /Users/jasonzhang/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "# classifier = TextClassifier.load('en-sentiment')\n",
    "classifier = TextClassifier.load(\"sentiment-fast\")\n",
    "\n",
    "def flair_sentiment(text):\n",
    "  # Return neutral if confidence < 0.75, value determined through hyperparameter tuning\n",
    "  sentence = Sentence(text)\n",
    "  classifier.predict(sentence)\n",
    "  if sentence.labels[0].score < 0.75:\n",
    "    return 1\n",
    "  return 0 if sentence.labels[0].value == \"NEGATIVE\" else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_sentiment(\"Ouch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'POSITIVE', 'confidence': 0.5508431196212769}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maybe label sentence as neutral if confidence < 0.65\n",
    "\n",
    "sentence=Sentence(\"Oh\")\n",
    "classifier.predict(sentence)\n",
    "sentence.labels\n",
    "sentence.labels[0].to_dict()\n",
    "# sentence.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    # return int(testimonial.sentiment.polarity>0.5)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if np.abs(polarity) < 0.15:\n",
    "        return 1\n",
    "    elif polarity < 0:\n",
    "        return 0\n",
    "    return 2\n",
    "\n",
    "textblob_sentiment(\"big car\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test\n",
    "* Use labeled Tweets dataset to evaluate each model\n",
    "* Track performance using:\n",
    "  * Classification metrics (precision, recall, etc.)\n",
    "  * Inference time (per tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>Haha thanks fansnim! happy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>Annapurna studios production no.29 on floors t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>I don't think you'd understand how much a stre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>Well</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>Worship me and spoil me happy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3351 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0                 How unhappy  some dogs like it though          0\n",
       "1     talking to my over driver about where I'm goin...          0\n",
       "2     Does anybody know if the Rand's likely to fall...          0\n",
       "3            I miss going to gigs in Liverpool unhappy           0\n",
       "4         There isnt a new Riverdale tonight ? unhappy           0\n",
       "...                                                 ...        ...\n",
       "3346                         Haha thanks fansnim! happy          2\n",
       "3347  Annapurna studios production no.29 on floors t...          2\n",
       "3348  I don't think you'd understand how much a stre...          2\n",
       "3349                                               Well          2\n",
       "3350                      Worship me and spoil me happy          2\n",
       "\n",
       "[3351 rows x 2 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test set\n",
    "test = pd.read_csv(\"data/test_set.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = pd.concat([test.iloc[0:50], test.iloc[1500:1550], test.iloc[2500:2550]])\n",
    "test_sample.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import time\n",
    "\n",
    "def performance_test(pred_func, text_data, true_labels, class_labels = [0,1,2]):\n",
    "    preds = []\n",
    "    fails = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for text in text_data:\n",
    "        try:\n",
    "            preds.append(pred_func(text))\n",
    "        except Exception as e:\n",
    "            print(e, text)\n",
    "            fails.append(text)\n",
    "    exec_time = time.time() - start_time\n",
    "    print(classification_report(true_labels, preds, labels=class_labels))\n",
    "    report = classification_report(true_labels, preds, labels=class_labels, output_dict=True)\n",
    "    return report, exec_time\n",
    "    # return f\"Total execution time: {exec_time}s, execution time per sample: {exec_time / len(text_data)}s\", report\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71      1117\n",
      "           1       0.68      0.79      0.73      1117\n",
      "           2       0.83      0.78      0.80      1117\n",
      "\n",
      "    accuracy                           0.75      3351\n",
      "   macro avg       0.75      0.75      0.75      3351\n",
      "weighted avg       0.75      0.75      0.75      3351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_report = performance_test(roberta_sentiment, test.text, test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.22      0.34      1117\n",
      "           1       0.42      0.94      0.58      1117\n",
      "           2       0.85      0.40      0.54      1117\n",
      "\n",
      "    accuracy                           0.52      3351\n",
      "   macro avg       0.69      0.52      0.49      3351\n",
      "weighted avg       0.69      0.52      0.49      3351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vader_report = performance_test(vader_sentiment, test.text, test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70      1117\n",
      "           1       0.53      0.37      0.44      1117\n",
      "           2       0.61      0.71      0.66      1117\n",
      "\n",
      "    accuracy                           0.61      3351\n",
      "   macro avg       0.60      0.61      0.60      3351\n",
      "weighted avg       0.60      0.61      0.60      3351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flair_report = performance_test(flair_sentiment, test.text, test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.66      0.76      1117\n",
      "           1       0.65      0.80      0.72      1117\n",
      "           2       0.77      0.80      0.78      1117\n",
      "\n",
      "    accuracy                           0.75      3351\n",
      "   macro avg       0.78      0.75      0.75      3351\n",
      "weighted avg       0.78      0.75      0.75      3351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textblob_report = performance_test(textblob_sentiment, test.text, test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': {'precision': 0.6220028208744711,\n",
       "   'recall': 0.7896150402864817,\n",
       "   'f1-score': 0.6958579881656805,\n",
       "   'support': 1117},\n",
       "  '1': {'precision': 0.5550755939524838,\n",
       "   'recall': 0.23008057296329454,\n",
       "   'f1-score': 0.32531645569620254,\n",
       "   'support': 1117},\n",
       "  '2': {'precision': 0.5782312925170068,\n",
       "   'recall': 0.7609668755595345,\n",
       "   'f1-score': 0.6571318129107073,\n",
       "   'support': 1117},\n",
       "  'accuracy': 0.5935541629364369,\n",
       "  'macro avg': {'precision': 0.5851032357813205,\n",
       "   'recall': 0.5935541629364369,\n",
       "   'f1-score': 0.5594354189241968,\n",
       "   'support': 3351},\n",
       "  'weighted avg': {'precision': 0.5851032357813206,\n",
       "   'recall': 0.5935541629364369,\n",
       "   'f1-score': 0.5594354189241968,\n",
       "   'support': 3351}},\n",
       " 5.092592000961304)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': {'0': {'precision': 0.753,\n",
       "   'recall': 0.6741271262309758,\n",
       "   'f1-score': 0.711384034010392,\n",
       "   'support': 1117},\n",
       "  '1': {'precision': 0.6787644787644788,\n",
       "   'recall': 0.7869292748433303,\n",
       "   'f1-score': 0.7288557213930349,\n",
       "   'support': 1117},\n",
       "  '2': {'precision': 0.8267045454545454,\n",
       "   'recall': 0.7815577439570277,\n",
       "   'f1-score': 0.8034974689369534,\n",
       "   'support': 1117},\n",
       "  'accuracy': 0.747538048343778,\n",
       "  'macro avg': {'precision': 0.752823008073008,\n",
       "   'recall': 0.7475380483437779,\n",
       "   'f1-score': 0.7479124081134602,\n",
       "   'support': 3351},\n",
       "  'weighted avg': {'precision': 0.7528230080730081,\n",
       "   'recall': 0.747538048343778,\n",
       "   'f1-score': 0.7479124081134602,\n",
       "   'support': 3351}},\n",
       " 'v': {'0': {'precision': 0.81,\n",
       "   'recall': 0.21754700089525514,\n",
       "   'f1-score': 0.3429781227946366,\n",
       "   'support': 1117},\n",
       "  '1': {'precision': 0.41570805236017455,\n",
       "   'recall': 0.9382273948075202,\n",
       "   'f1-score': 0.5761407366684992,\n",
       "   'support': 1117},\n",
       "  '2': {'precision': 0.8452830188679246,\n",
       "   'recall': 0.40107430617726053,\n",
       "   'f1-score': 0.5440194292653309,\n",
       "   'support': 1117},\n",
       "  'accuracy': 0.5189495672933453,\n",
       "  'macro avg': {'precision': 0.6903303570760331,\n",
       "   'recall': 0.5189495672933453,\n",
       "   'f1-score': 0.4877127629094889,\n",
       "   'support': 3351},\n",
       "  'weighted avg': {'precision': 0.6903303570760331,\n",
       "   'recall': 0.5189495672933453,\n",
       "   'f1-score': 0.4877127629094889,\n",
       "   'support': 3351}},\n",
       " 'f': {'0': {'precision': 0.6588235294117647,\n",
       "   'recall': 0.7520143240823635,\n",
       "   'f1-score': 0.7023411371237458,\n",
       "   'support': 1117},\n",
       "  '1': {'precision': 0.5293367346938775,\n",
       "   'recall': 0.37153088630259623,\n",
       "   'f1-score': 0.436612309310889,\n",
       "   'support': 1117},\n",
       "  '2': {'precision': 0.6130030959752322,\n",
       "   'recall': 0.7090420769919427,\n",
       "   'f1-score': 0.6575342465753423,\n",
       "   'support': 1117},\n",
       "  'accuracy': 0.6108624291256342,\n",
       "  'macro avg': {'precision': 0.6003877866936248,\n",
       "   'recall': 0.6108624291256342,\n",
       "   'f1-score': 0.5988292310033257,\n",
       "   'support': 3351},\n",
       "  'weighted avg': {'precision': 0.6003877866936249,\n",
       "   'recall': 0.6108624291256342,\n",
       "   'f1-score': 0.5988292310033257,\n",
       "   'support': 3351}},\n",
       " 't': {'0': {'precision': 0.9230769230769231,\n",
       "   'recall': 0.612354521038496,\n",
       "   'f1-score': 0.736275565123789,\n",
       "   'support': 1117},\n",
       "  '1': {'precision': 0.6154362416107383,\n",
       "   'recall': 0.8209489704565801,\n",
       "   'f1-score': 0.7034906022247794,\n",
       "   'support': 1117},\n",
       "  '2': {'precision': 0.7821428571428571,\n",
       "   'recall': 0.7842435094001791,\n",
       "   'f1-score': 0.7831917746982565,\n",
       "   'support': 1117},\n",
       "  'accuracy': 0.7391823336317517,\n",
       "  'macro avg': {'precision': 0.7735520072768395,\n",
       "   'recall': 0.7391823336317517,\n",
       "   'f1-score': 0.7409859806822751,\n",
       "   'support': 3351},\n",
       "  'weighted avg': {'precision': 0.7735520072768396,\n",
       "   'recall': 0.7391823336317517,\n",
       "   'f1-score': 0.740985980682275,\n",
       "   'support': 3351}}}"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports = [roberta_report, vader_report, flair_report, textblob_report]\n",
    "metrics = {model: model_metrics[0] for model, model_metrics in zip([\"r\", \"v\", \"f\", \"t\"], reports)}\n",
    "exec_times = {model: model_metrics[1] for model, model_metrics in zip([\"r\", \"v\", \"f\", \"t\"], reports)}\n",
    "metrics\n",
    "# exec_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: r, accuracy: 0.747538048343778\n",
      "Model: v, accuracy: 0.5189495672933453\n",
      "Model: f, accuracy: 0.6108624291256342\n",
      "Model: t, accuracy: 0.7391823336317517\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracies of 4 models\n",
    "for model, metric_report in metrics.items():\n",
    "    print(f\"Model: {model}, accuracy: {metric_report['accuracy']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the Roberta and Textblob models are far outperforming Vader and Flair in the accuracy measure. Because our test dataset was class-balanced, accuracy score is an effective metrics, so we can focus our analysis on the Roberta and Textblob models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'f1-score': 0.711384034010392,\n",
      "       'precision': 0.753,\n",
      "       'recall': 0.6741271262309758,\n",
      "       'support': 1117},\n",
      " '1': {'f1-score': 0.7288557213930349,\n",
      "       'precision': 0.6787644787644788,\n",
      "       'recall': 0.7869292748433303,\n",
      "       'support': 1117},\n",
      " '2': {'f1-score': 0.8034974689369534,\n",
      "       'precision': 0.8267045454545454,\n",
      "       'recall': 0.7815577439570277,\n",
      "       'support': 1117},\n",
      " 'accuracy': 0.747538048343778,\n",
      " 'macro avg': {'f1-score': 0.7479124081134602,\n",
      "               'precision': 0.752823008073008,\n",
      "               'recall': 0.7475380483437779,\n",
      "               'support': 3351},\n",
      " 'weighted avg': {'f1-score': 0.7479124081134602,\n",
      "                  'precision': 0.7528230080730081,\n",
      "                  'recall': 0.747538048343778,\n",
      "                  'support': 3351}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(roberta_report[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2be0c5224b2fc5679877ab994835d32e5e61031a6913e0f9d088633754b14faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
