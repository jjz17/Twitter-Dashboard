{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonzhang/Documents/PersonalProjects/twitter_dashboard/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 929/929 [00:00<00:00, 277kB/s]\n",
      "Downloading: 100%|██████████| 501M/501M [00:39<00:00, 12.7MB/s] \n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.76MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.22MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 41.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "\"\"\"\n",
    "Sentiment Analysis model and tokenizer\n",
    "\"\"\"\n",
    "\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess tweet text for the model\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word.startswith(\"@\") and len(word) > 1:\n",
    "            word = \"@user\"\n",
    "        elif word.startswith(\"http\"):\n",
    "            word = \"http\"\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    processed_text = preprocess_text(text)\n",
    "    # sentiment analysis\n",
    "    encoded_tweet = tokenizer(processed_text, return_tensors=\"pt\")\n",
    "    output = model(**encoded_tweet)\n",
    "\n",
    "    # Convert output pytorch tensor to numpy array by detaching the computational graph\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ind = argmax(scores)\n",
    "\n",
    "    for label, score in zip(labels, scores):\n",
    "        # print(f'\\t{label}: {score}')\n",
    "        pass\n",
    "\n",
    "    sentiment = labels[ind]\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look into producing inference on batched data with roberta model\n",
    "\n",
    "\n",
    "analyze_sentiment(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food was great!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentence = \"The food was great!\"\n",
    "vs = analyzer.polarity_scores(sentence)\n",
    "# print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "print(sentence)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 23:36:34,039 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-fasttext-rnn/sentiment-en-mix-ft-rnn_v8.pt not found in cache, downloading to /var/folders/q0/tp61rdx579s1pr09fjqsx_gw0000gn/T/tmp2r3knahj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241977025/1241977025 [01:44<00:00, 11855534.79B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 23:38:19,404 copying /var/folders/q0/tp61rdx579s1pr09fjqsx_gw0000gn/T/tmp2r3knahj to cache at /Users/jasonzhang/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 23:38:20,680 removing temp file /var/folders/q0/tp61rdx579s1pr09fjqsx_gw0000gn/T/tmp2r3knahj\n",
      "2023-03-03 23:38:20,784 loading file /Users/jasonzhang/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "# classifier = TextClassifier.load('en-sentiment')\n",
    "classifier = TextClassifier.load(\"sentiment-fast\")\n",
    "\n",
    "def text_sentiment_flair(text):\n",
    "  sentence = Sentence(text)\n",
    "  classifier.predict(sentence)\n",
    "  return np.round(sentence.labels[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment_flair(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence: \"The car is red\"'/'POSITIVE' (0.5098)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maybe label sentence as neutral if confidence < 0.6\n",
    "\n",
    "sentence=Sentence(\"The car is red\")\n",
    "classifier.predict(sentence)\n",
    "sentence.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def text_sentiment(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    # return int(testimonial.sentiment.polarity>0.5)\n",
    "    return testimonial.sentiment\n",
    "\n",
    "text_sentiment(\"Bad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2be0c5224b2fc5679877ab994835d32e5e61031a6913e0f9d088633754b14faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
